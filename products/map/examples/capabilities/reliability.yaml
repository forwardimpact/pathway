# yaml-language-server: $schema=https://www.forwardimpact.team/schema/json/capability.schema.json

id: reliability
name: Reliability
emojiIcon: üõ°Ô∏è
ordinalRank: 8
description: |
  Ensuring systems are dependable, secure, and observable.
  Includes DevOps practices, security, monitoring, incident response,
  and infrastructure management.
professionalResponsibilities:
  awareness:
    You follow security and operational guidelines, escalate issues
    appropriately, and participate in on-call rotations with guidance
  foundational:
    You implement reliability practices in your code, create basic monitoring,
    and contribute effectively to incident response
  working:
    You design for reliability, implement comprehensive monitoring and alerting,
    lead incident response, and drive post-incident improvements
  practitioner:
    You establish SLOs/SLIs across teams, build resilient systems, lead
    reliability initiatives for your area, mentor engineers on reliability
    practices, and drive reliability culture
  expert:
    You shape reliability strategy across the business unit, lead critical
    incident management, pioneer new reliability practices, and are the
    authority on system resilience
managementResponsibilities:
  awareness:
    You understand reliability requirements and support incident escalation
    processes
  foundational:
    You ensure your team follows reliability practices, manage on-call
    schedules, and facilitate incident retrospectives
  working:
    You own team reliability outcomes, manage incident response rotations, staff
    reliability initiatives, and champion operational excellence
  practitioner:
    You drive reliability culture across teams, establish SLOs and incident
    management processes for your area, and own cross-team reliability outcomes
  expert:
    You shape reliability strategy across the business unit, lead critical
    incident management at executive level, and own enterprise reliability
    outcomes
skills:
  - id: service_management
    name: Service Management
    isHumanOnly: true
    human:
      description:
        Managing services throughout their lifecycle from design to retirement,
        focusing on value delivery to users
      proficiencyDescriptions:
        awareness:
          You understand service lifecycle concepts (design, deploy, operate,
          retire) and follow service management processes established by others.
        foundational:
          You document services you own, participate in service reviews, handle
          basic service requests, and understand SLAs for your services.
        working:
          You design service offerings with clear value propositions, manage
          service level agreements, improve service delivery based on user
          feedback, and communicate service status proactively.
        practitioner:
          You lead service management practices for multiple services across
          teams, optimize service portfolios for your area, balance service
          investments, and train engineers on service-oriented thinking.
        expert:
          You shape service management strategy across the business unit. You
          drive service excellence culture, innovate on service delivery
          approaches, and are recognized as a service management authority.
  - id: sre_practices
    name: Site Reliability Engineering
    human:
      description:
        Ensuring system reliability through observability, incident response,
        and capacity planning
      proficiencyDescriptions:
        awareness:
          You understand SLIs, SLOs, and error budgets conceptually. You can use
          monitoring dashboards and escalate issues appropriately.
        foundational:
          You create basic alerts and dashboards. You participate in on-call
          rotations and contribute to incident response under guidance.
        working:
          You design observability strategies for your services, lead incident
          response, implement resilience testing, and conduct blameless
          post-mortems. You balance reliability investment with feature
          velocity.
        practitioner:
          You define reliability standards across teams in your area, drive
          post-incident improvements systematically, design capacity planning
          processes, and mentor engineers on SRE practices.
        expert:
          You shape reliability culture and standards across the business unit.
          You pioneer new reliability practices, solve large-scale reliability
          challenges, and are recognized as an authority on system resilience.
    agent:
      name: sre-practices
      description: |
        Guide for ensuring system reliability through observability, incident
        response, and capacity planning.
      useWhen: |
        Designing monitoring, handling incidents, setting SLOs, or improving
        system resilience.
      stages:
        specify:
          focus: |
            Define reliability requirements and SLO targets.
            Identify critical user journeys that need protection.
          readChecklist:
            - Identify critical user journeys and business impact
            - Document reliability requirements (availability, latency)
            - Define SLO targets with stakeholder agreement
            - Specify acceptable error budgets
            - Mark ambiguities with [NEEDS CLARIFICATION]
          confirmChecklist:
            - Critical user journeys are identified
            - Reliability requirements are documented
            - SLO targets are defined
            - Error budgets are agreed
        plan:
          focus: |
            Define reliability requirements, SLIs/SLOs, and observability
            strategy. Plan for resilience and capacity needs.
          readChecklist:
            - Define SLIs for key user journeys
            - Set SLOs with stakeholder agreement
            - Plan observability strategy (metrics, logs, traces)
            - Identify failure modes and resilience patterns
            - Define alerting thresholds
          confirmChecklist:
            - SLIs defined for key user journeys
            - SLOs set with stakeholder agreement
            - Monitoring strategy is planned
            - Failure modes are identified
            - Alerting thresholds are defined
        onboard:
          focus: |
            Set up the observability and reliability tooling. Install
            tracing, logging, and monitoring libraries, and configure
            the observability backend.
          readChecklist:
            - Install observability tools (OpenTelemetry, Pino/structlog)
            - Configure OTLP exporter endpoint and credentials
            - Set up structured logging configuration
            - Verify traces and logs reach the observability backend
            - Create runbook template directory
          confirmChecklist:
            - OpenTelemetry SDK installed and configured
            - OTLP exporter sends data to backend
            - Structured logging produces valid JSON
            - Test trace and log entries appear in backend
            - Runbook directory structure created
        code:
          focus: |
            Implement observability, resilience patterns, and operational
            tooling. Build systems that fail gracefully and recover quickly.
          readChecklist:
            - Implement metrics, logging, and tracing
            - Configure alerts based on SLOs
            - Implement resilience patterns (timeouts, retries, circuit
              breakers)
            - Create runbooks for common issues
            - Set up error budget tracking
          confirmChecklist:
            - Comprehensive monitoring is in place
            - Alerts are actionable and low-noise
            - Resilience patterns are implemented
            - Runbooks exist for common issues
            - Error budget tracking is in place
        review:
          focus: |
            Verify reliability implementation meets SLOs and operational
            readiness. Ensure incident response procedures are in place.
          readChecklist:
            - Validate SLOs are measurable
            - Test failure scenarios
            - Review runbook completeness
            - Verify incident response procedures
            - Check alert quality and coverage
          confirmChecklist:
            - SLOs are measurable and validated
            - Failure scenarios are tested
            - Incident response process documented
            - Post-mortem culture established
            - Disaster recovery approach is tested
        deploy:
          focus: |
            Deploy reliability infrastructure and verify production
            monitoring. Ensure on-call readiness.
          readChecklist:
            - Deploy monitoring and alerting to production
            - Verify dashboards and alerts work correctly
            - Confirm on-call rotation is ready
            - Run production readiness review
          confirmChecklist:
            - Monitoring is live in production
            - Alerts fire correctly for SLO breaches
            - On-call team is trained and ready
            - Production readiness review is complete
    toolReferences:
      - name: OpenTelemetry
        url: https://opentelemetry.io/docs/
        simpleIcon: opentelemetry
        description:
          Vendor-neutral observability framework for traces, metrics, and logs
        useWhen:
          Instrumenting applications for distributed tracing and observability
      - name: Pino
        url: https://getpino.io/
        simpleIcon: nodedotjs
        description: Fast, low-overhead structured logging for Node.js
        useWhen: Adding structured logging to JavaScript applications
      - name: structlog
        url: https://www.structlog.org/
        simpleIcon: python
        description: Structured logging library for Python
        useWhen: Adding structured logging to Python applications
    instructions: |
      ## Step 1: Define SLIs and SLOs

      Identify what matters to users. For each critical user
      journey (page load, checkout, API), define an SLI (what to
      measure) and SLO (target threshold). Calculate error budgets
      from SLOs.

      ## Step 2: Instrument Application

      Add distributed tracing with OpenTelemetry. Configure the
      OTLP exporter to send traces to your observability backend.
      Use auto-instrumentation for common libraries.

      ## Step 3: Add Structured Logging

      Replace free-text logging with structured JSON logs. Use Pino
      for Node.js or structlog for Python. Always include context
      fields (userId, orderId, correlationId) for queryability.

      ## Step 4: Create Runbook Template

      For each alert, document symptoms, diagnosis steps,
      mitigation actions, and escalation criteria. Keep runbooks
      co-located with the alerting configuration.
    installScript: |
      set -e
      npm install @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node
      npm install @opentelemetry/exporter-trace-otlp-http pino
      pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp structlog
      python -c "import opentelemetry; import structlog"
    implementationReference: |
      ## SLI/SLO Table

      | User Journey | SLI | SLO |
      |--------------|-----|-----|
      | Page load | Latency p99 | < 500ms for 99.9% of requests |
      | Checkout | Success rate | > 99.95% of transactions succeed |
      | API | Availability | 99.9% uptime (43 min/month budget) |

      ## OpenTelemetry ‚Äî Node.js

      ```javascript
      const { NodeSDK } = require('@opentelemetry/sdk-node')
      const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
      const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')

      const sdk = new NodeSDK({
        traceExporter: new OTLPTraceExporter({
          url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces'
        }),
        instrumentations: [getNodeAutoInstrumentations()]
      })
      sdk.start()
      ```

      ## OpenTelemetry ‚Äî Python

      ```python
      from opentelemetry import trace
      from opentelemetry.sdk.trace import TracerProvider
      from opentelemetry.sdk.trace.export import BatchSpanProcessor
      from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

      provider = TracerProvider()
      provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter()))
      trace.set_tracer_provider(provider)
      ```

      ## Structured Logging ‚Äî Pino (Node.js)

      ```javascript
      const pino = require('pino')
      const logger = pino({ level: process.env.LOG_LEVEL || 'info' })

      logger.info({ userId: 123, action: 'checkout' }, 'Processing checkout')
      logger.error({ err, orderId }, 'Checkout failed')
      ```

      ## Structured Logging ‚Äî structlog (Python)

      ```python
      import structlog

      structlog.configure(processors=[
          structlog.processors.TimeStamper(fmt="iso"),
          structlog.processors.JSONRenderer()
      ])
      logger = structlog.get_logger()
      logger.info("processing_checkout", user_id=123)
      ```

      ## Runbook Template

      ```markdown
      # Runbook: High Error Rate

      ## Symptoms
      - Error rate > 0.1% for 5+ minutes

      ## Diagnosis
      1. Check application logs for error patterns
      2. Check dependency health endpoints
      3. Check recent deployments

      ## Mitigation
      1. If recent deploy: Roll back
      2. If dependency issue: Enable circuit breaker
      3. If load spike: Scale up

      ## Escalation
      If not resolved in 15 min, escalate to team lead.
      ```

      ## Verification

      Your reliability setup is working when:
      - Traces appear in your observability backend
      - Structured logs contain consistent fields (correlation IDs, user context)
      - Runbooks exist for known failure modes
      - Team knows how to respond to common alerts

      ## Common Pitfalls

      - **Missing environment variables**: OTLP exporters fail silently
      - **No correlation IDs**: Cannot trace requests across services
      - **Unstructured logs**: Free-text logs are hard to query
      - **Alert fatigue**: Too many alerts drown out real issues
      - **No runbooks**: Alerts fire but responders don't know what to do
